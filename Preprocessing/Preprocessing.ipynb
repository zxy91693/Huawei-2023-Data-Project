{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "功能: 合并两个dict\n",
    "input: \n",
    "    dict1\n",
    "    dict2\n",
    "output: \n",
    "    合并后dict\n",
    "'''\n",
    "def dict_union(d1, d2):\n",
    "    keys = d1.keys() | d2.keys()\n",
    "    temp = {}\n",
    "    for key in keys:\n",
    "        temp[key] = sum([d.get(key,0) for d in (d1, d2)])\n",
    "    return temp\n",
    "\n",
    "\n",
    "'''\n",
    "功能：统计dataframe信息\n",
    "input: \n",
    "    dataframe\n",
    "output: \n",
    "    dict{'U_id': value_counts.to_dict(), 'I_id': value_counts.to_dict(), 'label': value_counts.to_dict(), 'interactions': #interactions}\n",
    "'''\n",
    "def count_data_info(df):\n",
    "    statistic_dict = {}\n",
    "    for i in ['U_id', 'I_id', 'label']:\n",
    "        data_feature_num = df[i].value_counts(normalize = False, dropna = True).to_dict()        \n",
    "        statistic_dict[i] = data_feature_num\n",
    "    statistic_dict['interactions'] = np.sum(np.asarray(np.array(list(statistic_dict['U_id'].items()))[:,1], dtype = int))\n",
    "    return statistic_dict\n",
    "\n",
    "\n",
    "'''\n",
    "功能: 筛选warm users warm items / warm users cold items / cold users warm items / cold users cold items\n",
    "input: \n",
    "    被筛选dataframe\n",
    "    train dataframe的统计信息count_data_info(train_df)\n",
    "output: \n",
    "    筛选后只包含warm users warm items / warm users cold items / cold users warm items / cold users cold items的dataframe\n",
    "'''\n",
    "def warm_user_warm_item(df,train_statistic_dict):\n",
    "    df_out = df[(df['U_id'].isin(train_statistic_dict['U_id'])) & (df['I_id'].isin(train_statistic_dict['I_id']))].copy()\n",
    "    df_out.reset_index(drop = True,inplace = True)\n",
    "    return df_out\n",
    "\n",
    "def warm_user_cold_item(df,train_statistic_dict):\n",
    "    df_out = df[(df['U_id'].isin(train_statistic_dict['U_id'])) & (~df['I_id'].isin(train_statistic_dict['I_id']))].copy()\n",
    "    df_out.reset_index(drop = True,inplace = True)\n",
    "    return df_out\n",
    "\n",
    "def cold_user_warm_item(df,train_statistic_dict):\n",
    "    df_out = df[(~df['U_id'].isin(train_statistic_dict['U_id'])) & (df['I_id'].isin(train_statistic_dict['I_id']))].copy()\n",
    "    df_out.reset_index(drop = True,inplace = True)\n",
    "    return df_out\n",
    "\n",
    "def cold_user_cold_item(df,train_statistic_dict):\n",
    "    df_out = df[(~df['U_id'].isin(train_statistic_dict['U_id'])) & (~df['I_id'].isin(train_statistic_dict['I_id']))].copy()\n",
    "    df_out.reset_index(drop = True,inplace = True)\n",
    "    return df_out\n",
    "\n",
    "\n",
    "'''\n",
    "功能: 读取/生成并保存 ui_to_id_dict, id_to_ui_dict\n",
    "input: \n",
    "    not_sparse_dict {'U_id': set(Uid), 'I_id': set(Iid)}\n",
    "    target_path 读取/保存 路径\n",
    "output: \n",
    "    ui_to_id_dict {'U_id':{Uid:mapped_Uid}, 'I_id':{Iid:mapped_Iid}}\n",
    "    id_to_ui_dict {'U_id':{mapped_Uid:Uid}, 'I_id':{mapped_Iid:Iid}}\n",
    "'''\n",
    "def save_ui_dict(not_sparse_dict,target_path):\n",
    "    if os.path.exists(target_path + '/ui_to_id.dict') and os.path.exists(target_path + '/id_to_ui.dict'):\n",
    "        print('loading from file')\n",
    "        with open(target_path + '/ui_to_id.dict','rb') as f:\n",
    "            ui_to_id_dict = pickle.load(f)\n",
    "        with open(target_path + '/id_to_ui.dict','rb') as f:\n",
    "            id_to_ui_dict = pickle.load(f)\n",
    "    else:\n",
    "        ui_to_id_dict = {'U_id':{},'I_id':{}}\n",
    "\n",
    "        for user in list(not_sparse_dict['U_id']):\n",
    "            if user not in ui_to_id_dict['U_id'].keys():\n",
    "                ui_to_id_dict['U_id'][user] = len(ui_to_id_dict['U_id'])\n",
    "\n",
    "        for item in list(not_sparse_dict['I_id']):\n",
    "            if item not in ui_to_id_dict['I_id'].keys():\n",
    "                ui_to_id_dict['I_id'][item] = len(ui_to_id_dict['I_id'])\n",
    "\n",
    "        dict_file = open(target_path + '/ui_to_id.dict', 'wb') \n",
    "        pickle.dump(ui_to_id_dict,dict_file)\n",
    "        dict_file.close()\n",
    "\n",
    "        id_to_ui_dict = {'U_id':{v:k for k,v in ui_to_id_dict['U_id'].items()},'I_id':{v:k for k,v in ui_to_id_dict['I_id'].items()}}\n",
    "\n",
    "        dict_file = open(target_path + '/id_to_ui.dict', 'wb') \n",
    "        pickle.dump(id_to_ui_dict,dict_file)\n",
    "        dict_file.close()\n",
    "\n",
    "        dict_file = open(target_path + '/dataset_size.txt', 'w') \n",
    "        dict_file.write(\"{} {}\".format(str(len(ui_to_id_dict['U_id'])),str(len(ui_to_id_dict['I_id']))))\n",
    "        dict_file.close()\n",
    "    \n",
    "    return ui_to_id_dict,id_to_ui_dict\n",
    "\n",
    "\n",
    "'''\n",
    "功能: 将dataframe转换为 U_id,I_id 通过 ui_to_id_dict map过后的 Uid list(Iid) 形式分别保存正负样本\n",
    "input: \n",
    "    dataframe\n",
    "    ui_to_id_dict {'U_id':{Uid:mapped_Uid}, 'I_id':{Iid:mapped_Iid}}\n",
    "    pos_traget_path,neg_traget_path 保存路径\n",
    "'''\n",
    "def df_to_uipair_txt(df,ui_to_id_dict,pos_traget_path,neg_traget_path):\n",
    "    df_temp = df.copy()\n",
    "    df_temp['U_id'] = df_temp['U_id'].apply(lambda x:ui_to_id_dict['U_id'][x])\n",
    "    df_temp['I_id'] = df_temp['I_id'].apply(lambda x:ui_to_id_dict['I_id'][x])\n",
    "    \n",
    "    df_temp = df_temp.astype(str)\n",
    "    df_temp = df_temp.groupby(['U_id','label'], as_index=False).apply(lambda x: '\\t'.join(x['I_id']))\n",
    "    df_temp = pd.DataFrame(df_temp,columns=['I_id']).reset_index()\n",
    "    df_temp[df_temp['label']=='1'][['U_id','I_id']].to_csv(pos_traget_path, header=None, index=False, sep='\\t', quoting=csv.QUOTE_NONE, escapechar=' ')\n",
    "    df_temp[df_temp['label']=='0'][['U_id','I_id']].to_csv(neg_traget_path, header=None, index=False, sep='\\t', quoting=csv.QUOTE_NONE, escapechar=' ')\n",
    "\n",
    "\n",
    "'''\n",
    "功能: 将dataframe转换为uipair_dict\n",
    "input: \n",
    "    dataframe\n",
    "    uipair_dict {U_id: list(I_id)}\n",
    "'''\n",
    "def df_to_uipair_dict(df):\n",
    "    df_temp = df.copy()[['U_id','I_id']]\n",
    "    uipair_dict = df_temp.groupby('U_id')['I_id'].apply(list).to_dict()\n",
    "    return uipair_dict\n",
    "\n",
    "\n",
    "'''\n",
    "功能: 递归筛选item和user使得保留下来的每个item和user均拥有不小于k的interaction\n",
    "input: \n",
    "    interaction_dict {U_id: list(I_id)}\n",
    "    k 阈值\n",
    "output:\n",
    "    interaction_dict 筛选后的interaction_dict\n",
    "    len(interaction_dict) 筛选后user个数\n",
    "    item_num  筛选item个数\n",
    "'''           \n",
    "def select_kcore(interaction_dict,K=3):\n",
    "    flag = 0\n",
    "    while flag==0:\n",
    "        item_cnt_dict = {}\n",
    "        item_drop_dict = {}\n",
    "        # create item_drop_dict, item_cnt_dict\n",
    "        for user_id in interaction_dict:\n",
    "            for item_id in interaction_dict[user_id]:\n",
    "                if item_id not in item_cnt_dict:\n",
    "                    item_cnt_dict[item_id] = 0\n",
    "                    item_drop_dict[item_id] = 0\n",
    "                item_cnt_dict[item_id] += 1\n",
    "\n",
    "        #print('user num:',len(interaction_dict))\n",
    "        assert len(item_drop_dict)==len(item_cnt_dict)\n",
    "\n",
    "        # delete items < K\n",
    "        del_iid_list = []\n",
    "        for i_id in item_cnt_dict:\n",
    "            if item_cnt_dict[i_id] < K:\n",
    "                del_iid_list.append(i_id)\n",
    "\n",
    "        for i_id in del_iid_list:\n",
    "            item_drop_dict[i_id] = 1\n",
    "        for u_id in interaction_dict:\n",
    "            del_id_list = []\n",
    "            for i_id in interaction_dict[u_id]:\n",
    "                if item_drop_dict[i_id]:\n",
    "                    del_id_list.append(i_id)\n",
    "            for del_id in del_id_list:\n",
    "                if del_id in interaction_dict[u_id]:\n",
    "                    interaction_dict[u_id].remove(del_id)\n",
    "\n",
    "        item_drop_num = 0\n",
    "        for i_id in item_drop_dict:\n",
    "            item_drop_num += item_drop_dict[i_id]\n",
    "        item_num = len(item_drop_dict) - item_drop_num\n",
    "        #print(f'item num after item-{K}core:',item_num)\n",
    "\n",
    "        new_item_cnt = {}\n",
    "        min_cnt=999\n",
    "        for u_id in interaction_dict:\n",
    "            min_cnt = min(min_cnt, len(interaction_dict[u_id]))\n",
    "            for i_id in interaction_dict[u_id]:\n",
    "                if i_id not in new_item_cnt:\n",
    "                    new_item_cnt[i_id] = 0\n",
    "                new_item_cnt[i_id] += 1\n",
    "        #print('min user interaction:',min_cnt)\n",
    "        min_cnt_item = 999\n",
    "        for i_id in new_item_cnt:\n",
    "            min_cnt_item = min(min_cnt_item, new_item_cnt[i_id])\n",
    "        #print('min item num:',min_cnt_item)\n",
    "        if min_cnt>=K and min_cnt_item>=K:\n",
    "            return interaction_dict, len(interaction_dict), item_num\n",
    "        \n",
    "        # delete users interactions<K\n",
    "        del_uid_list = []\n",
    "        for u_id in interaction_dict:\n",
    "            if len(interaction_dict[u_id])<K:\n",
    "                del_uid_list.append(u_id)\n",
    "        for u_id in del_uid_list:\n",
    "            del interaction_dict[u_id]\n",
    "        \n",
    "        # count min user-interaction and item appearance\n",
    "        new_item_cnt = {}\n",
    "        min_cnt=999\n",
    "        for u_id in interaction_dict:\n",
    "            min_cnt = min(min_cnt, len(interaction_dict[u_id]))\n",
    "            for i_id in interaction_dict[u_id]:\n",
    "                if i_id not in new_item_cnt:\n",
    "                    new_item_cnt[i_id] = 0\n",
    "                new_item_cnt[i_id] += 1\n",
    "        min_cnt_item = 999\n",
    "        for i_id in new_item_cnt:\n",
    "            min_cnt_item = min(min_cnt_item, new_item_cnt[i_id])\n",
    "            \n",
    "        if min_cnt>=K and min_cnt_item>=K:\n",
    "            return interaction_dict, len(interaction_dict), item_num\n",
    "\n",
    "\n",
    "'''\n",
    "功能: 调用select_kcore筛选 user item, 并得到筛选后的dataframe\n",
    "input: \n",
    "    datframe\n",
    "    interaction_dict {U_id: list(I_id)}\n",
    "    k 阈值\n",
    "output:\n",
    "    df_remove_sparse 筛选后的dataframe\n",
    "    not_sparse_dict {'U_id': set(Uid), 'I_id': set(Iid)}筛选后留下的user item\n",
    "'''  \n",
    "def remove_sparse_interaction(df,uipair_dict,K=3):\n",
    "    interaction_dict, not_sparse_user, not_sparse_item = select_kcore(uipair_dict,K)\n",
    "    \n",
    "    not_sparse_dict = {'U_id':set([]),'I_id':set([])}\n",
    "    \n",
    "    for k,v in interaction_dict.items():\n",
    "        not_sparse_dict['U_id'].add(k)\n",
    "        for i in v:\n",
    "            not_sparse_dict['I_id'].add(i)\n",
    "\n",
    "    df_remove_sparse = df[(df['U_id'].isin(not_sparse_dict['U_id'])) & (df['I_id'].isin(not_sparse_dict['I_id']))].copy()\n",
    "    return df_remove_sparse, not_sparse_dict\n",
    "\n",
    "\n",
    "'''\n",
    "功能: 读取Uid list(Iid) txt文件 转化为ui_pairs_dict\n",
    "input: \n",
    "    filename 路径\n",
    "output:\n",
    "    ui_pairs_dict {Uid: list(Iid)}\n",
    "'''\n",
    "def get_dict(filename):\n",
    "    ui_pairs_dict = {}\n",
    "    for line in open(filename):\n",
    "        each = [int(x) for x in line.strip().split(\"\\t\")]\n",
    "        ui_pairs_dict[each[0]] = each[1:]\n",
    "    return ui_pairs_dict\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unique clean raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取原始dataframe数据\n",
    "base_path = \"/storage/wjwang/share/Huawei/Huawei_data_2023\"\n",
    "dcn_data_raw = pd.read_csv(base_path + '/dcn_data.txt', sep='\\t', header=None)\n",
    "\n",
    "# drop掉前3列\n",
    "dcn_data_pro_df = dcn_data_raw.drop(columns=[0,1,2],inplace = False)\n",
    "# 从第49列中读取出 U_id I_id date label 信息, 拆分保存\n",
    "dcn_data_pro_df[['date', 'label','U_id','I_id','Other']]= dcn_data_pro_df[48].str.split('#', expand=True)\n",
    "dcn_data_pro_df.drop(columns=[48,'Other'],inplace = True)\n",
    "\n",
    "# 筛选(U,I,label)第一天, 只保留相同 U_id I_id label 重复的第一天的信息\n",
    "extracted_raw_unique_df = dcn_data_pro_df.groupby(['U_id', 'I_id', 'label'], as_index=False).apply(lambda x: min(x['date']))\n",
    "extracted_raw_unique_df = pd.DataFrame(extracted_raw_unique_df,columns=['date'])\n",
    "extracted_raw_unique_df = extracted_raw_unique_df.reset_index()\n",
    "\n",
    "# 筛选(U,I,label)第一天中正样本, 只保留相同 U_id I_id 交互中正样本的信息\n",
    "temp_splited_unique = extracted_raw_unique_df.groupby(['U_id', 'I_id'], as_index=False)['label'].max()\n",
    "temp_splited_unique = pd.DataFrame(temp_splited_unique,columns=['U_id', 'I_id', 'label'])\n",
    "temp_splited_unique = temp_splited_unique.reset_index()[['U_id', 'I_id', 'label']]\n",
    "extracted_raw_clean_df = pd.merge(temp_splited_unique,extracted_raw_unique_df,on = ['U_id', 'I_id', 'label'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## item_feature concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取item_feature dataframe并根据 I_id 对应拼接\n",
    "\n",
    "item_features_raw = pd.read_csv(base_path+'/item_feature.txt', sep='\\t')\n",
    "\n",
    "item_features_raw_temp = item_features_raw.rename(columns={'doc_id':'I_id'},inplace=False)\n",
    "\n",
    "dcn_item_concated_df = pd.merge(dcn_raw_clean_df_temp,item_features_raw_temp,on = ['I_id'], how = 'left')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 掉 huawei 未使用的feature\n",
    "dcn_item_concated_df.drop(columns=[32,'static_quality_score'],inplace = True) \n",
    "\n",
    "# mask 掉出现次数小于10的稀疏 feature value, 0.01%的概率 mask 掉一些 feature value\n",
    "mincount = 10\n",
    "maskrate = 0.01\n",
    "mask_feature_dict = {}\n",
    "\n",
    "if os.path.exists('./mincount{}_maskrate{}_mask_feature.dict'.format(str(mincount),str(maskrate))):\n",
    "    print('loading from file')\n",
    "    with open('./mincount{}_maskrate{}_mask_feature.dict'.format(str(mincount),str(maskrate)),'rb') as f:\n",
    "        mask_feature_dict = pickle.load(f)\n",
    "else:\n",
    "    print('generating mask_feature_dict')\n",
    "    for i in raw_feature_analyse_dict.keys():\n",
    "        if i not in [7,30,31,34,32,48]:\n",
    "            if i not in [3,4]:\n",
    "                mask_feature_dict[i] = {}\n",
    "                for k in list(raw_feature_analyse_dict[i]['info'][raw_feature_analyse_dict[i]['info'].values < mincount].index): \n",
    "                    mask_feature_dict[i][k] = 'mincount_feature'\n",
    "                for k in list(raw_feature_analyse_dict[i]['info'][raw_feature_analyse_dict[i]['info'].values >= mincount].index): \n",
    "                    if random.randint(1,10000)<= maskrate*10000:\n",
    "                        mask_feature_dict[i][k] = 'maskrate_feature'\n",
    "            else:\n",
    "                mask_feature_dict[i] = {}\n",
    "                for k in list(raw_feature_analyse_dict[i]['info'][raw_feature_analyse_dict[i]['info'].values < mincount].index): \n",
    "                    mask_feature_dict[i][k] = -999\n",
    "                for k in list(raw_feature_analyse_dict[i]['info'][raw_feature_analyse_dict[i]['info'].values >= mincount].index): \n",
    "                    if random.randint(1,10000)<= maskrate*10000:\n",
    "                        mask_feature_dict[i][k] = -9999\n",
    "\n",
    "    for i in item_feature_analyse_dict.keys():\n",
    "        if i not in ['static_quality_score']:\n",
    "            if i not in ['ori_cpid', 'exposure_times', 'ctr_score']:\n",
    "                mask_feature_dict[i] = {}\n",
    "                for k in list(item_feature_analyse_dict[i]['info'][item_feature_analyse_dict[i]['info'].values < mincount].index):\n",
    "                    mask_feature_dict[i][k] = 'mincount_feature'\n",
    "                for k in list(item_feature_analyse_dict[i]['info'][item_feature_analyse_dict[i]['info'].values >= mincount].index):\n",
    "                    if random.randint(1,10000)<= maskrate*10000:\n",
    "                        mask_feature_dict[i][k] = 'maskrate_feature'\n",
    "            else:\n",
    "                mask_feature_dict[i] = {}\n",
    "                for k in list(item_feature_analyse_dict[i]['info'][item_feature_analyse_dict[i]['info'].values < mincount].index):\n",
    "                    mask_feature_dict[i][k] = -999\n",
    "                for k in list(item_feature_analyse_dict[i]['info'][item_feature_analyse_dict[i]['info'].values >= mincount].index):\n",
    "                    if random.randint(1,10000)<= maskrate*10000:\n",
    "                        mask_feature_dict[i][k] = -9999\n",
    "\n",
    "\n",
    "    statistic_dict_file = open('./mincount{}_maskrate{}_mask_feature.dict'.format(str(mincount),str(maskrate)), 'wb') \n",
    "    pickle.dump(mask_feature_dict,statistic_dict_file)\n",
    "    statistic_dict_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 部分连续变量按照 huawei 方提供的分段值分段分类处理\n",
    "bins = {34:[-1,0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,10.0,20.0,30.0,9999],\\\n",
    "        31:[-1,0,1.01,1.25,1.34,1.5,2.0,2.32,3.0,3.5,4.0,4.49,4.99,5.99,7.99,10.49,33.53,49.59,9999],\\\n",
    "        30:[-1,0,54.76,63.08,64.13,64.95,70.94,74.05,74.43,75.15,75.71,9999],\\\n",
    "        7:[-1,0,1.66,3.37,10.26,12.91,14.59,25.71,27.85,42.29,73.34,9999],\\\n",
    "       }\n",
    "\n",
    "default_replace_dict = {30:{'user_feed_avg_doc_max_completion_daily_x':'-1'},\\\n",
    "                  31:{'user_feed_avg_vv_daily_x':'-1'},\\\n",
    "                  34:{'user_last_used_since_today_days_x':'-1'}}\n",
    "\n",
    "dcn_item_concated_df.replace(default_replace_dict,inplace=True)\n",
    "\n",
    "dcn_item_concated_df[[7,30,31,34]] = dcn_item_concated_df[[7,30,31,34]].astype(float)\n",
    "\n",
    "for k,v in bins.items():\n",
    "    dcn_item_concated_df[k] = pd.cut(dcn_item_concated_df[k],v,include_lowest=True,right = False,labels = [i for i in range(len(v)-1)])\n",
    "\n",
    "for k,v in mask_feature_dict.items():\n",
    "    print('Replacing ',k,':')\n",
    "    start_time = time.time()\n",
    "    df_mat = dcn_item_concated_df[k].values\n",
    "    df_mat[:] = [v[x] if x in v else x for x in df_mat]\n",
    "    dcn_item_concated_df.loc[:,k]=df_mat\n",
    "    end_time = time.time()\n",
    "    print('time cost:',float(end_time-start_time)*1000,'ms')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 去除sparse user, 拆分train valid test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去除掉 sparse 的user, 即交互数量小于等于 sparse 的 user\n",
    "dcn_data_clean_7days_dropped = dcn_item_concated_df.copy()\n",
    "\n",
    "top=0\n",
    "sparse = 10\n",
    "\n",
    "uid_inter_count_7days_dropped = dcn_data_clean_7days_dropped.groupby('U_id',as_index=False).count()\n",
    "uid_inter_count_7days_dropped = uid_inter_count_7days_dropped[['U_id','I_id']]\n",
    "uid_inter_count_7days_dropped.rename(columns={'U_id':'U_id','I_id': 'count'}, inplace=True)\n",
    "uid_inter_count_7days_dropped = uid_inter_count_7days_dropped[uid_inter_count_7days_dropped['count']>sparse]\n",
    "dcn_data_clean_7days_dropped_sparse = dcn_data_clean_7days_dropped.loc[dcn_data_clean_7days_dropped['U_id'].isin(uid_inter_count_7days_dropped['U_id'].values)]\n",
    "\n",
    "dcn_data_clean_7days_dropped_sparse['date'] = pd.to_datetime(dcn_data_clean_7days_dropped_sparse['date'],format='%Y%m%d')\n",
    "\n",
    "# 设定导出数据集的路径\n",
    "target_path = '/storage/shjing/xinyang/Huawei_data_2023/datasets/feature-0125-0131_itemfeature_mincount{}_maskrate{}_total_usersparse{}_top{}_temp'.format(str(mincount),str(maskrate),str(sparse),str(top))\n",
    "if not os.path.exists(target_path):\n",
    "    os.makedirs(target_path)\n",
    "\n",
    "df_alldays_unique_remove_sparse = dcn_data_clean_7days_dropped_sparse.copy()\n",
    "\n",
    "# k-core, 删掉sparse user和item, 返回str id的 dataframe和ui交互dict\n",
    "remove_top_uipair_alldays_dict = df_to_uipair_dict(df_alldays_unique_remove_sparse)\n",
    "df_alldays_unique_remove_sparse, not_sparse_dict = remove_sparse_interaction(df_alldays_unique_remove_sparse,remove_top_uipair_alldays_dict,0)\n",
    "\n",
    "# 生成map和reverse map\n",
    "ui_to_id_dict_alldays_sparse,id_to_ui_dict_alldays_sparse = save_ui_dict(not_sparse_dict,target_path)\n",
    "\n",
    "df_alldays_unique_statistic_dict = count_data_info(df_alldays_unique_remove_sparse)\n",
    "statistic_dict_file = open(target_path + '/df_all_unique_statistic.dict', 'wb') \n",
    "pickle.dump(df_alldays_unique_statistic_dict,statistic_dict_file)\n",
    "statistic_dict_file.close()\n",
    "print(\"Final Dataset:\")\n",
    "print(\"interactions:\",df_alldays_unique_statistic_dict[\"interactions\"],\"\\nusers:\",len(df_alldays_unique_statistic_dict['U_id']),\"\\nitems:\",len(df_alldays_unique_statistic_dict['I_id']),\"\\nlabels:\",df_alldays_unique_statistic_dict['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start_time =  pd.Timestamp('2023-01-25')\n",
    "train_end_time = pd.Timestamp('2023-01-29')\n",
    "val_start_time = pd.Timestamp(\"2023-01-30\")\n",
    "val_end_time = pd.Timestamp(\"2023-01-30\")\n",
    "test_start_time = pd.Timestamp(\"2023-01-31\")\n",
    "test_end_time = pd.Timestamp(\"2023-01-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 数据集构建\n",
    "df_train = df_alldays_unique_remove_sparse[(df_alldays_unique_remove_sparse['date']>=train_start_time) & (df_alldays_unique_remove_sparse['date']<=train_end_time)].copy()\n",
    "\n",
    "df_alldays_train_statistic_dict = count_data_info(df_train)\n",
    "statistic_dict_file = open(target_path + '/df_train_statistic.dict', 'wb') \n",
    "pickle.dump(df_alldays_train_statistic_dict,statistic_dict_file)\n",
    "statistic_dict_file.close()\n",
    "print()\n",
    "print(\"train:\")\n",
    "print(\"interactions:\",df_alldays_train_statistic_dict[\"interactions\"],\"\\nusers:\",len(df_alldays_train_statistic_dict['U_id']),\"\\nitems:\",len(df_alldays_train_statistic_dict['I_id']),\"\\nlabels:\",df_alldays_train_statistic_dict['label'])\n",
    "\n",
    "df_to_uipair_txt(df_train,ui_to_id_dict_alldays_sparse,target_path+'/train_pos.txt',target_path+'/train_neg.txt')\n",
    "\n",
    "\n",
    "# valid 数据集构建\n",
    "# valid\n",
    "df_val = df_alldays_unique_remove_sparse[(df_alldays_unique_remove_sparse['date']>=val_start_time) & (df_alldays_unique_remove_sparse['date']<=val_end_time)].copy()\n",
    "\n",
    "df_alldays_val_statistic_dict = count_data_info(df_val)\n",
    "statistic_dict_file = open(target_path + '/df_val_statistic.dict', 'wb') \n",
    "pickle.dump(df_alldays_val_statistic_dict,statistic_dict_file)\n",
    "statistic_dict_file.close()\n",
    "print()\n",
    "print(\"val:\")\n",
    "print(\"interactions:\",df_alldays_val_statistic_dict[\"interactions\"],\"\\nusers:\",len(df_alldays_val_statistic_dict['U_id']),\"\\nitems:\",len(df_alldays_val_statistic_dict['I_id']),\"\\nlabels:\",df_alldays_val_statistic_dict['label'])\n",
    "\n",
    "df_to_uipair_txt(df_val,ui_to_id_dict_alldays_sparse,target_path+'/val_pos.txt',target_path+'/val_neg.txt')\n",
    "\n",
    "# valid cold user cold item\n",
    "df_cuci_val = cold_user_cold_item(df_val,df_alldays_train_statistic_dict)\n",
    "\n",
    "df_alldays_val_statistic_dict = count_data_info(df_cuci_val)\n",
    "statistic_dict_file = open(target_path + '/df_cuci_val_statistic.dict', 'wb') \n",
    "pickle.dump(df_alldays_val_statistic_dict,statistic_dict_file)\n",
    "statistic_dict_file.close()\n",
    "print()\n",
    "print(\"val_cuci:\")\n",
    "print(\"interactions:\",df_alldays_val_statistic_dict[\"interactions\"],\"\\nusers:\",len(df_alldays_val_statistic_dict['U_id']),\"\\nitems:\",len(df_alldays_val_statistic_dict['I_id']),\"\\nlabels:\",df_alldays_val_statistic_dict['label'])\n",
    "\n",
    "df_to_uipair_txt(df_cuci_val,ui_to_id_dict_alldays_sparse,target_path+'/cuci_val_pos.txt',target_path+'/cuci_val_neg.txt')\n",
    "\n",
    "# valid warm user cold item\n",
    "df_wuci_val = warm_user_cold_item(df_val,df_alldays_train_statistic_dict)\n",
    "\n",
    "df_alldays_val_statistic_dict = count_data_info(df_wuci_val)\n",
    "statistic_dict_file = open(target_path + '/df_wuci_val_statistic.dict', 'wb') \n",
    "pickle.dump(df_alldays_val_statistic_dict,statistic_dict_file)\n",
    "statistic_dict_file.close()\n",
    "print()\n",
    "print(\"val_wuci:\")\n",
    "print(\"interactions:\",df_alldays_val_statistic_dict[\"interactions\"],\"\\nusers:\",len(df_alldays_val_statistic_dict['U_id']),\"\\nitems:\",len(df_alldays_val_statistic_dict['I_id']),\"\\nlabels:\",df_alldays_val_statistic_dict['label'])\n",
    "\n",
    "df_to_uipair_txt(df_wuci_val,ui_to_id_dict_alldays_sparse,target_path+'/wuci_val_pos.txt',target_path+'/wuci_val_neg.txt')\n",
    "\n",
    "# valid cold user warm item\n",
    "df_cuwi_val = cold_user_warm_item(df_val,df_alldays_train_statistic_dict)\n",
    "\n",
    "df_alldays_val_statistic_dict = count_data_info(df_cuwi_val)\n",
    "statistic_dict_file = open(target_path + '/df_cuwi_val_statistic.dict', 'wb') \n",
    "pickle.dump(df_alldays_val_statistic_dict,statistic_dict_file)\n",
    "statistic_dict_file.close()\n",
    "print()\n",
    "print(\"val_cuwi:\")\n",
    "print(\"interactions:\",df_alldays_val_statistic_dict[\"interactions\"],\"\\nusers:\",len(df_alldays_val_statistic_dict['U_id']),\"\\nitems:\",len(df_alldays_val_statistic_dict['I_id']),\"\\nlabels:\",df_alldays_val_statistic_dict['label'])\n",
    "\n",
    "df_to_uipair_txt(df_cuwi_val,ui_to_id_dict_alldays_sparse,target_path+'/cuwi_val_pos.txt',target_path+'/cuwi_val_neg.txt')\n",
    "\n",
    "# valid warm user warm item\n",
    "df_wuwi_val = warm_user_warm_item(df_val,df_alldays_train_statistic_dict)\n",
    "\n",
    "df_alldays_val_statistic_dict = count_data_info(df_wuwi_val)\n",
    "statistic_dict_file = open(target_path + '/df_wuwi_val_statistic.dict', 'wb') \n",
    "pickle.dump(df_alldays_val_statistic_dict,statistic_dict_file)\n",
    "statistic_dict_file.close()\n",
    "print()\n",
    "print(\"val_wuwi:\")\n",
    "print(\"interactions:\",df_alldays_val_statistic_dict[\"interactions\"],\"\\nusers:\",len(df_alldays_val_statistic_dict['U_id']),\"\\nitems:\",len(df_alldays_val_statistic_dict['I_id']),\"\\nlabels:\",df_alldays_val_statistic_dict['label'])\n",
    "\n",
    "df_to_uipair_txt(df_wuwi_val,ui_to_id_dict_alldays_sparse,target_path+'/wuwi_val_pos.txt',target_path+'/wuwi_val_neg.txt')\n",
    "\n",
    "\n",
    "# test 数据集构建\n",
    "# test\n",
    "df_test = df_alldays_unique_remove_sparse[(df_alldays_unique_remove_sparse['date']>=test_start_time) & (df_alldays_unique_remove_sparse['date']<=test_end_time)].copy()\n",
    "\n",
    "df_alldays_test_statistic_dict = count_data_info(df_test)\n",
    "statistic_dict_file = open(target_path + '/df_test_statistic.dict', 'wb') \n",
    "pickle.dump(df_alldays_test_statistic_dict,statistic_dict_file)\n",
    "statistic_dict_file.close()\n",
    "print()\n",
    "print(\"test:\")\n",
    "print(\"interactions:\",df_alldays_test_statistic_dict[\"interactions\"],\"\\nusers:\",len(df_alldays_test_statistic_dict['U_id']),\"\\nitems:\",len(df_alldays_test_statistic_dict['I_id']),\"\\nlabels:\",df_alldays_test_statistic_dict['label'])\n",
    "\n",
    "df_to_uipair_txt(df_test,ui_to_id_dict_alldays_sparse,target_path+'/test_pos.txt',target_path+'/test_neg.txt')\n",
    "\n",
    "# valid cold user cold item\n",
    "df_cuci_test = cold_user_cold_item(df_test,df_alldays_train_statistic_dict)\n",
    "\n",
    "df_alldays_test_statistic_dict = count_data_info(df_cuci_test)\n",
    "statistic_dict_file = open(target_path + '/df_cuci_test_statistic.dict', 'wb') \n",
    "pickle.dump(df_alldays_test_statistic_dict,statistic_dict_file)\n",
    "statistic_dict_file.close()\n",
    "print()\n",
    "print(\"test_cuci:\")\n",
    "print(\"interactions:\",df_alldays_test_statistic_dict[\"interactions\"],\"\\nusers:\",len(df_alldays_test_statistic_dict['U_id']),\"\\nitems:\",len(df_alldays_test_statistic_dict['I_id']),\"\\nlabels:\",df_alldays_test_statistic_dict['label'])\n",
    "\n",
    "df_to_uipair_txt(df_cuci_test,ui_to_id_dict_alldays_sparse,target_path+'/cuci_test_pos.txt',target_path+'/cuci_test_neg.txt')\n",
    "\n",
    "# valid warm user cold item\n",
    "df_wuci_test = warm_user_cold_item(df_test,df_alldays_train_statistic_dict)\n",
    "\n",
    "df_alldays_test_statistic_dict = count_data_info(df_wuci_test)\n",
    "statistic_dict_file = open(target_path + '/df_wuci_test_statistic.dict', 'wb') \n",
    "pickle.dump(df_alldays_test_statistic_dict,statistic_dict_file)\n",
    "statistic_dict_file.close()\n",
    "print()\n",
    "print(\"test_wuci:\")\n",
    "print(\"interactions:\",df_alldays_test_statistic_dict[\"interactions\"],\"\\nusers:\",len(df_alldays_test_statistic_dict['U_id']),\"\\nitems:\",len(df_alldays_test_statistic_dict['I_id']),\"\\nlabels:\",df_alldays_test_statistic_dict['label'])\n",
    "\n",
    "df_to_uipair_txt(df_wuci_test,ui_to_id_dict_alldays_sparse,target_path+'/wuci_test_pos.txt',target_path+'/wuci_test_neg.txt')\n",
    "\n",
    "# valid cold user warm item\n",
    "df_cuwi_test = cold_user_warm_item(df_test,df_alldays_train_statistic_dict)\n",
    "\n",
    "df_alldays_test_statistic_dict = count_data_info(df_cuwi_test)\n",
    "statistic_dict_file = open(target_path + '/df_cuwi_test_statistic.dict', 'wb') \n",
    "pickle.dump(df_alldays_test_statistic_dict,statistic_dict_file)\n",
    "statistic_dict_file.close()\n",
    "print()\n",
    "print(\"test_cuwi:\")\n",
    "print(\"interactions:\",df_alldays_test_statistic_dict[\"interactions\"],\"\\nusers:\",len(df_alldays_test_statistic_dict['U_id']),\"\\nitems:\",len(df_alldays_test_statistic_dict['I_id']),\"\\nlabels:\",df_alldays_test_statistic_dict['label'])\n",
    "\n",
    "df_to_uipair_txt(df_cuwi_test,ui_to_id_dict_alldays_sparse,target_path+'/cuwi_test_pos.txt',target_path+'/cuwi_test_neg.txt')\n",
    "\n",
    "# valid warm user warm item\n",
    "df_wuwi_test = warm_user_warm_item(df_test,df_alldays_train_statistic_dict)\n",
    "\n",
    "df_alldays_test_statistic_dict = count_data_info(df_wuwi_test)\n",
    "statistic_dict_file = open(target_path + '/df_wuwi_test_statistic.dict', 'wb') \n",
    "pickle.dump(df_alldays_test_statistic_dict,statistic_dict_file)\n",
    "statistic_dict_file.close()\n",
    "print()\n",
    "print(\"test_wuwi:\")\n",
    "print(\"interactions:\",df_alldays_test_statistic_dict[\"interactions\"],\"\\nusers:\",len(df_alldays_test_statistic_dict['U_id']),\"\\nitems:\",len(df_alldays_test_statistic_dict['I_id']),\"\\nlabels:\",df_alldays_test_statistic_dict['label'])\n",
    "\n",
    "df_to_uipair_txt(df_wuwi_test,ui_to_id_dict_alldays_sparse,target_path+'/wuwi_test_pos.txt',target_path+'/wuwi_test_neg.txt')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unnormalized popularity 计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stage 信息统计\n",
    "print('generating stage_info_dict')\n",
    "stage_info_dict = {}\n",
    "train_stage = ['stage_'+str(i) for i in range(5)]\n",
    "valid_stage = ['stage_'+str(i) for i in range(3,5)]\n",
    "test_stage = ['stage_'+str(i) for i in range(4,6)]\n",
    "\n",
    "stage_info_dict['train'] = train_stage\n",
    "stage_info_dict['valid'] = valid_stage\n",
    "stage_info_dict['test'] = test_stage\n",
    "\n",
    "statistic_dict_file = open(target_path + '/stage_info_dict.dict', 'wb') \n",
    "pickle.dump(stage_info_dict,statistic_dict_file)\n",
    "statistic_dict_file.close()\n",
    "\n",
    "stage_days = [pd.Timestamp('2023-01-'+str(i)) for i in range(25,31)]\n",
    "stage_to_day_map_dict = {}\n",
    "day_to_stage_map_dict = {}\n",
    "for i in range(len(stage_days)):\n",
    "    stage_to_day_map_dict['stage_'+str(i)] = stage_days[i]\n",
    "    day_to_stage_map_dict[stage_days[i]] = 'stage_'+str(i)\n",
    "\n",
    "\n",
    "# 生成 stage_item_popularity_dict {stage:{I_id: popularity}}\n",
    "if os.path.exists(target_path + '/stage_item_popularity_dict.dict'):\n",
    "    print('loading from file')\n",
    "    with open(target_path + '/stage_item_popularity_dict.dict','rb') as f:\n",
    "        stage_item_popularity_dict = pickle.load(f)\n",
    "else:\n",
    "    print('generating stage_item_popularity_dict')\n",
    "    stage_item_popularity_dict = {}\n",
    "    for i in range(len(stage_days)):\n",
    "        selected_day = stage_days[i]\n",
    "        df_oneday = df_alldays_unique_remove_sparse[['U_id','I_id']][df_alldays_unique_remove_sparse['date']==selected_day].copy()\n",
    "        iid_inter_count_oneday = df_oneday.groupby('I_id',as_index=False).count()\n",
    "        iid_inter_count_oneday = iid_inter_count_oneday[['U_id','I_id']]\n",
    "        iid_inter_count_oneday.rename(columns={'I_id':'I_id','U_id': 'count_'+ str(i)}, inplace=True)\n",
    "        interactions_oneday = iid_inter_count_oneday['count_'+ str(i)].sum()\n",
    "        iid_inter_count_oneday['count_'+ str(i)] = iid_inter_count_oneday['count_'+ str(i)]/interactions_oneday\n",
    "        iid_inter_count_oneday.set_index(['I_id'], inplace = True)\n",
    "        stage_item_popularity_dict['stage_'+str(i)] = iid_inter_count_oneday.to_dict('dict')['count_'+ str(i)]\n",
    "\n",
    "        statistic_dict_file = open(target_path + '/stage_item_popularity_dict.dict', 'wb') \n",
    "        pickle.dump(stage_item_popularity_dict,statistic_dict_file)\n",
    "        statistic_dict_file.close()\n",
    "\n",
    "# 生成 map 后 item id 对应的stage popularity stage_iid_popularity_dict {stage:{Iid: popularity}}\n",
    "print('generating stage_iid_popularity_dict')\n",
    "stage_iid_popularity_dict = {}\n",
    "for k,v in stage_item_popularity_dict.items():\n",
    "    stage_iid_popularity_dict[k] = {}\n",
    "    for k1,v1 in v.items():\n",
    "        stage_iid_popularity_dict[k][ui_to_id_dict_alldays_sparse['I_id'][k1]] = v1\n",
    "\n",
    "statistic_dict_file = open(target_path + '/stage_iid_popularity_dict.dict', 'wb') \n",
    "pickle.dump(stage_iid_popularity_dict,statistic_dict_file)\n",
    "statistic_dict_file.close()\n",
    "\n",
    "\n",
    "# 生成 stage 默认值dict stage_default_popularity_dict {stage: default_popularity}\n",
    "if os.path.exists(target_path + '/stage_default_popularity_dict.dict'):\n",
    "    print('loading from file')\n",
    "    with open(target_path + '/stage_default_popularity_dict.dict','rb') as f:\n",
    "        stage_default_popularity_dict = pickle.load(f)\n",
    "else:\n",
    "    print('generating stage_default_popularity_dict')\n",
    "    stage_default_popularity_dict = {}\n",
    "    for i in range(len(stage_days)):\n",
    "        stage_default_popularity_dict['stage_'+str(i)] = 1/len(stage_item_popularity_dict['stage_'+str(i)])\n",
    "\n",
    "    statistic_dict_file = open(target_path + '/stage_default_popularity_dict.dict', 'wb') \n",
    "    pickle.dump(stage_default_popularity_dict,statistic_dict_file)\n",
    "    statistic_dict_file.close()\n",
    "\n",
    "\n",
    "# 生成 map 后 (uid, iid) pair对应的 popularity (处理后的数据集 ui pair 对应的时间是唯一的) mappedui_to_popularity_dict {(uid,iid):popularitys}\n",
    "temp_df_uidate_popular = df_alldays_unique_remove_sparse_encoded[['U_id','I_id','date']]\n",
    "\n",
    "if os.path.exists(target_path + '/mappedui_to_popularity_dict.dict'):\n",
    "    print('loading from file')\n",
    "    with open(target_path + '/mappedui_to_popularity_dict.dict','rb') as f:\n",
    "        mappedui_to_popularity_dict = pickle.load(f)\n",
    "else:\n",
    "    print('generating mappedui_to_popularity_dict')\n",
    "    mappedui_to_popularity_dict = {}\n",
    "    for idx, row in temp_df_uidate_popular.iterrows():\n",
    "        if row['date'] in day_to_stage_map_dict:\n",
    "             mappedui_to_popularity_dict[(ui_to_id_dict_alldays_sparse['U_id'][row['U_id']],ui_to_id_dict_alldays_sparse['I_id'][row['I_id']])] = stage_item_popularity_dict[day_to_stage_map_dict[row['date']]][row['I_id']]\n",
    "\n",
    "    statistic_dict_file = open(target_path + '/mappedui_to_popularity_dict.dict', 'wb') \n",
    "    pickle.dump(mappedui_to_popularity_dict,statistic_dict_file)\n",
    "    statistic_dict_file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalized popularity 计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(target_path):\n",
    "    os.makedirs(target_path)\n",
    "\n",
    "print('generating stage_info_dict')\n",
    "stage_info_dict = {}\n",
    "train_stage = ['stage_'+str(i) for i in range(5)]\n",
    "valid_stage = ['stage_'+str(i) for i in range(3,5)]\n",
    "test_stage = ['stage_'+str(i) for i in range(4,6)]\n",
    "\n",
    "stage_info_dict['train'] = train_stage\n",
    "stage_info_dict['valid'] = valid_stage\n",
    "stage_info_dict['test'] = test_stage\n",
    "\n",
    "statistic_dict_file = open(target_path + '/stage_info_dict.dict', 'wb') \n",
    "pickle.dump(stage_info_dict,statistic_dict_file)\n",
    "statistic_dict_file.close()\n",
    "\n",
    "stage_days = [pd.Timestamp('2023-01-'+str(i)) for i in range(25,31)]\n",
    "stage_to_day_map_dict = {}\n",
    "day_to_stage_map_dict = {}\n",
    "for i in range(len(stage_days)):\n",
    "    stage_to_day_map_dict['stage_'+str(i)] = stage_days[i]\n",
    "    day_to_stage_map_dict[stage_days[i]] = 'stage_'+str(i)\n",
    "\n",
    "df_train_valid = df_alldays_unique_remove_sparse[(df_alldays_unique_remove_sparse['date']>=train_start_time) & (df_alldays_unique_remove_sparse['date']<=val_end_time)].copy()\n",
    "\n",
    "train_iid = df_train_valid['I_id'].values\n",
    "n_items = len(train_iid)\n",
    "\n",
    "\n",
    "if os.path.exists(target_path + '/stage_item_popularity_dict.dict'):\n",
    "    print('loading from file')\n",
    "    with open(target_path + '/stage_item_popularity_dict.dict','rb') as f:\n",
    "        stage_item_popularity_dict = pickle.load(f)\n",
    "else:\n",
    "    print('generating stage_item_popularity_dict')\n",
    "    stage_item_popularity_dict = {}\n",
    "    interactions_stage = {}\n",
    "    for i in range(len(stage_days)):\n",
    "        selected_day = stage_days[i]\n",
    "        df_oneday = df_alldays_unique_remove_sparse[['U_id','I_id']][df_alldays_unique_remove_sparse['date']==selected_day].copy()\n",
    "        iid_inter_count_oneday = df_oneday.groupby('I_id',as_index=False).count()\n",
    "        iid_inter_count_oneday = iid_inter_count_oneday[['U_id','I_id']]\n",
    "        iid_inter_count_oneday.rename(columns={'I_id':'I_id','U_id': 'count_'+ str(i)}, inplace=True)\n",
    "        interactions_oneday = iid_inter_count_oneday['count_'+ str(i)].sum()\n",
    "        interactions_stage['stage_'+str(i)] = interactions_oneday\n",
    "        iid_inter_count_oneday['count_'+ str(i)] = iid_inter_count_oneday['count_'+ str(i)]\n",
    "        iid_inter_count_oneday.set_index(['I_id'], inplace = True)\n",
    "        stage_item_popularity_dict['stage_'+str(i)] = iid_inter_count_oneday.to_dict('dict')['count_'+ str(i)]\n",
    "    \n",
    "    temp_dict = {}\n",
    "    for k,v in stage_item_popularity_dict.items():\n",
    "        temp_dict[k] = {}\n",
    "        for iid in train_iid:\n",
    "            if iid in v:\n",
    "                temp_dict[k][iid] = (1+stage_item_popularity_dict[k][iid])/(interactions_stage[k]+n_items)\n",
    "            else:\n",
    "                temp_dict[k][iid] = 1/(interactions_stage[k]+n_items)\n",
    "    stage_item_popularity_dict=temp_dict.copy()\n",
    "    \n",
    "    deno_normal_dict = {}\n",
    "    temp_dict = {}\n",
    "    for k,v in stage_item_popularity_dict.items():\n",
    "        temp_dict[k] = {}\n",
    "        deno_normal = max(list(v.values())) - min(list(v.values()))\n",
    "        min_value = min(list(v.values()))\n",
    "        deno_normal_dict[k] = deno_normal\n",
    "        for k1,v1 in v.items():\n",
    "            temp_dict[k][k1] = (v1-min_value)/deno_normal\n",
    "    stage_item_popularity_dict=temp_dict.copy()\n",
    "        \n",
    "    statistic_dict_file = open(target_path + '/stage_item_popularity_dict.dict', 'wb') \n",
    "    pickle.dump(stage_item_popularity_dict,statistic_dict_file)\n",
    "    statistic_dict_file.close()\n",
    "\n",
    "\n",
    "print('generating stage_iid_popularity_dict')\n",
    "stage_iid_popularity_dict = {}\n",
    "for k,v in stage_item_popularity_dict.items():\n",
    "    stage_iid_popularity_dict[k] = {}\n",
    "    for k1,v1 in v.items():\n",
    "        stage_iid_popularity_dict[k][ui_to_id_dict_alldays_sparse['I_id'][k1]] = v1\n",
    "\n",
    "statistic_dict_file = open(target_path + '/stage_iid_popularity_dict.dict', 'wb') \n",
    "pickle.dump(stage_iid_popularity_dict,statistic_dict_file)\n",
    "statistic_dict_file.close()\n",
    "\n",
    "if os.path.exists(target_path + '/stage_default_popularity_dict.dict'):\n",
    "    print('loading from file')\n",
    "    with open(target_path + '/stage_default_popularity_dict.dict','rb') as f:\n",
    "        stage_default_popularity_dict = pickle.load(f)\n",
    "else:\n",
    "    print('generating stage_default_popularity_dict')\n",
    "    stage_default_popularity_dict = {}\n",
    "    for i in range(len(stage_days)):\n",
    "        stage_default_popularity_dict['stage_'+str(i)] = 0\n",
    "        \n",
    "        \n",
    "    statistic_dict_file = open(target_path + '/stage_default_popularity_dict.dict', 'wb') \n",
    "    pickle.dump(stage_default_popularity_dict,statistic_dict_file)\n",
    "    statistic_dict_file.close()\n",
    "\n",
    "\n",
    "temp_df_uidate_popular = df_alldays_unique_remove_sparse_encoded[['U_id','I_id','date']]\n",
    "\n",
    "if os.path.exists(target_path + '/mappedui_to_popularity_dict.dict'):\n",
    "    print('loading from file')\n",
    "    with open(target_path + '/mappedui_to_popularity_dict.dict','rb') as f:\n",
    "        mappedui_to_popularity_dict = pickle.load(f)\n",
    "else:\n",
    "    print('generating mappedui_to_popularity_dict')\n",
    "    mappedui_to_popularity_dict = {}\n",
    "    for idx, row in temp_df_uidate_popular.iterrows():\n",
    "        if row['date'] in day_to_stage_map_dict:\n",
    "            ui_index = (ui_to_id_dict_alldays_sparse['U_id'][row['U_id']],ui_to_id_dict_alldays_sparse['I_id'][row['I_id']])\n",
    "            mappedui_to_popularity_dict[ui_index] = stage_item_popularity_dict[day_to_stage_map_dict[row['date']]][row['I_id']]\n",
    "\n",
    "    statistic_dict_file = open(target_path + '/mappedui_to_popularity_dict.dict', 'wb') \n",
    "    pickle.dump(mappedui_to_popularity_dict,statistic_dict_file)\n",
    "    statistic_dict_file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重新map feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新对每个 feature 的 feature value 进行 map\n",
    "def label_encode_columns(df, columns):\n",
    "    encoders = {}\n",
    "    for col in columns:\n",
    "        le = preprocessing.LabelEncoder().fit(df[col])\n",
    "        df[col] = le.transform(df[col])\n",
    "        encoders[col] = le\n",
    "    return df, encoders\n",
    "\n",
    "def label_encode_columns_w_fit_encoders(df, columns, encoders):\n",
    "    for col in columns:\n",
    "        le = encoders.get(col)\n",
    "        df[col] = le.transform(df[col])\n",
    "    return df\n",
    "\n",
    "encode_columns =  [ 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,\n",
    "                    26, 27, 28, 29,  30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47,\n",
    "                    'entities',  'ori_cpid', 'category_tags', 'tags', 'topics', 'category_entities',\n",
    "                    'category',  'multi_label', 'doc_publish_time', 'title_tags', 'exposure_times', 'ctr_score']\n",
    "\n",
    "df_alldays_unique_remove_sparse_encoded, encoders = label_encode_columns(df=df_alldays_unique_remove_sparse, columns=encode_columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导出feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_feat_prefix = {'U_id':'U',7:'UFrefreshn',8:'UFgender',\\\n",
    "                 9:'UFforecast_age',10:'UFdevice_price',\\\n",
    "                 11:'UFprovince_dev',12:'UFpush_online_days_30d_dev',13:'UFpush_online_days_7d_dev',\\\n",
    "                 14:'UFcity_new_dev',15:'UFcity_new_grade_dev',\\\n",
    "                 16:'UFuser_feed_click_doc_cats_30days',17:'UFuser_feed_click_doc_topics_30days',\\\n",
    "                 18:'UFuser_feed_click_doc_keys_30days',19:'UFuser_feed_click_doc_entities_30days',\\\n",
    "                 20:'UFuser_feed_read_doc_cats_30days',21:'UFuser_feed_read_doc_topics_30days',\\\n",
    "                 22:'UFuser_feed_read_doc_keys_30days',23:'user_feed_read_doc_entities_30days',\\\n",
    "                 24:'UFuser_feed_click_svideo_cats_30days',25:'UFuser_feed_click_svideo_keys_30days',\\\n",
    "                 26:'UFuser_feed_play_svideo_cats_30days',27:'UFuser_feed_play_svideo_keys_30days',\\\n",
    "                 28:'UFuser_feed_user_life_cycle',\\\n",
    "                 29:'UFuser_feed_avg_doc_reading_daily_bin',30:'UFuser_feed_avg_doc_max_completion_daily',\\\n",
    "                 31:'UFuser_feed_avg_vv_daily',33:'UFuser_feed_avg_refresh_daily',\\\n",
    "                 34:'UFuser_last_used_since_today_days',35:'UFuser_feed_explore_doc_cats',\\\n",
    "                 36:'UFuser_feed_explore_doc_topics',37:'UFuser_feed_explore_doc_keys',\\\n",
    "                 38:'UFuser_feed_click_app_cats',\\\n",
    "                }\n",
    "\n",
    "i_feat_prefix = {'I_id':'I',3:'IFdtype',4:'IFacinnerpos',\\\n",
    "                 5:'IFe_ch',6:'IFsource',\\\n",
    "                 39:'IFentities',40:'IFcategory_tags',41:'IFtags',\\\n",
    "                 42:'IFtopics',43:'IFcategory_entities',\\\n",
    "                 44:'IFmanual_category',45:'IFstatic_quality_score_bin',\\\n",
    "                 46:'IFctr_score_bin',47:'IFexposure_times_bin',\\\n",
    "                 'entities':'IFNewentities','ori_cpid':'IFNewori_cpid',\\\n",
    "                 'category_tags':'IFNewcategory_tags','tags':'IFNewtags','topics':'IFNewtopics',\\\n",
    "                 'category_entities':'IFNewcategory_entities','category':'IFNewcategory',\\\n",
    "                 'multi_label':'IFNewmulti_label', 'doc_publish_time':'IFNewdoc_publish_time',\\\n",
    "                 'title_tags':'IFNewtitle_tags', 'exposure_times':'IFNewexposure_times', 'ctr_score':'IFNewctr_score',\\\n",
    "                }\n",
    "\n",
    "ctx_feat_prefix = {}\n",
    "\n",
    "\n",
    "# 生成导出所有user,item的feature文件\n",
    "print()\n",
    "print('Start generating feature file...')\n",
    "user_feature = {} \n",
    "item_feature = {} \n",
    "ctx_feature = {}\n",
    "\n",
    "for idx, row in df_alldays_unique_remove_sparse_encoded.iterrows():\n",
    "    if ui_to_id_dict_alldays_sparse['U_id'][row['U_id']] not in user_feature:\n",
    "        user_feature[ui_to_id_dict_alldays_sparse['U_id'][row['U_id']]] = [[],[]]\n",
    "        for column in u_feat_prefix.keys():\n",
    "            if column == 'U_id':\n",
    "                user_feature[ui_to_id_dict_alldays_sparse['U_id'][row['U_id']]][0].append('U'+str(ui_to_id_dict_alldays_sparse['U_id'][row['U_id']]))\n",
    "                user_feature[ui_to_id_dict_alldays_sparse['U_id'][row['U_id']]][1].append(1)\n",
    "            else:\n",
    "                user_feature[ui_to_id_dict_alldays_sparse['U_id'][row['U_id']]][0].append(u_feat_prefix[column]+str(row[column]))\n",
    "                user_feature[ui_to_id_dict_alldays_sparse['U_id'][row['U_id']]][1].append(1)\n",
    "                \n",
    "    if ui_to_id_dict_alldays_sparse['I_id'][row['I_id']] not in item_feature:\n",
    "        item_feature[ui_to_id_dict_alldays_sparse['I_id'][row['I_id']]] = [[],[]]\n",
    "        for column in i_feat_prefix.keys():\n",
    "            if column == 'I_id':\n",
    "                item_feature[ui_to_id_dict_alldays_sparse['I_id'][row['I_id']]][0].append('I'+str(ui_to_id_dict_alldays_sparse['I_id'][row['I_id']]))\n",
    "                item_feature[ui_to_id_dict_alldays_sparse['I_id'][row['I_id']]][1].append(1)\n",
    "            else:\n",
    "                item_feature[ui_to_id_dict_alldays_sparse['I_id'][row['I_id']]][0].append(i_feat_prefix[column]+str(row[column]))\n",
    "                item_feature[ui_to_id_dict_alldays_sparse['I_id'][row['I_id']]][1].append(1)\n",
    "                \n",
    "    if (ui_to_id_dict_alldays_sparse['U_id'][row['U_id']],ui_to_id_dict_alldays_sparse['I_id'][row['I_id']]) not in item_feature:\n",
    "        \n",
    "        ctx_feature[(ui_to_id_dict_alldays_sparse['U_id'][row['U_id']],\\\n",
    "                     ui_to_id_dict_alldays_sparse['I_id'][row['I_id']])] = [[],[]]\n",
    "        for column in ctx_feat_prefix.keys():\n",
    "            ctx_feature[(ui_to_id_dict_alldays_sparse['U_id'][row['U_id']],\\\n",
    "                         ui_to_id_dict_alldays_sparse['I_id'][row['I_id']])][0].append(ctx_feat_prefix[column]+str(row[column]))\n",
    "\n",
    "            ctx_feature[(ui_to_id_dict_alldays_sparse['U_id'][row['U_id']],\\\n",
    "                         ui_to_id_dict_alldays_sparse['I_id'][row['I_id']])][1].append(1)\n",
    "        \n",
    "np.save(target_path+'/user_feature.npy',user_feature)\n",
    "np.save(target_path+'/item_feature.npy',item_feature)\n",
    "np.save(target_path+'/ctx_feature.npy',ctx_feature)\n",
    "print('Generating feature file done.')\n",
    "\n",
    "# 从txt导成npy的dict\n",
    "print()\n",
    "print('Start generating interaction dict.npy...')\n",
    "\n",
    "train_pos = get_dict(target_path+'/train_pos.txt')\n",
    "train_neg = get_dict(target_path+'/train_neg.txt')\n",
    "\n",
    "np.save(target_path+'/training_pos.npy',train_pos)\n",
    "np.save(target_path+'/training_neg.npy',train_neg)\n",
    "\n",
    "valid_pos = get_dict(target_path+'/cuci_val_pos.txt')\n",
    "valid_neg = get_dict(target_path+'/cuci_val_neg.txt')\n",
    "test_pos = get_dict(target_path+'/cuci_test_pos.txt')\n",
    "test_neg = get_dict(target_path+'/cuci_test_neg.txt')\n",
    "np.save(target_path+'/cuci_validation_pos.npy',valid_pos)\n",
    "np.save(target_path+'/cuci_validation_neg.npy',valid_neg)\n",
    "np.save(target_path+'/cuci_testing_pos.npy',test_pos)\n",
    "np.save(target_path+'/cuci_testing_neg.npy',test_neg)\n",
    "\n",
    "valid_pos = get_dict(target_path+'/wuci_val_pos.txt')\n",
    "valid_neg = get_dict(target_path+'/wuci_val_neg.txt')\n",
    "test_pos = get_dict(target_path+'/wuci_test_pos.txt')\n",
    "test_neg = get_dict(target_path+'/wuci_test_neg.txt')\n",
    "np.save(target_path+'/wuci_validation_pos.npy',valid_pos)\n",
    "np.save(target_path+'/wuci_validation_neg.npy',valid_neg)\n",
    "np.save(target_path+'/wuci_testing_pos.npy',test_pos)\n",
    "np.save(target_path+'/wuci_testing_neg.npy',test_neg)\n",
    "\n",
    "valid_pos = get_dict(target_path+'/cuwi_val_pos.txt')\n",
    "valid_neg = get_dict(target_path+'/cuwi_val_neg.txt')\n",
    "test_pos = get_dict(target_path+'/cuwi_test_pos.txt')\n",
    "test_neg = get_dict(target_path+'/cuwi_test_neg.txt')\n",
    "np.save(target_path+'/cuwi_validation_pos.npy',valid_pos)\n",
    "np.save(target_path+'/cuwi_validation_neg.npy',valid_neg)\n",
    "np.save(target_path+'/cuwi_testing_pos.npy',test_pos)\n",
    "np.save(target_path+'/cuwi_testing_neg.npy',test_neg)\n",
    "\n",
    "valid_pos = get_dict(target_path+'/wuwi_val_pos.txt')\n",
    "valid_neg = get_dict(target_path+'/wuwi_val_neg.txt')\n",
    "test_pos = get_dict(target_path+'/wuwi_test_pos.txt')\n",
    "test_neg = get_dict(target_path+'/wuwi_test_neg.txt')\n",
    "np.save(target_path+'/wuwi_validation_pos.npy',valid_pos)\n",
    "np.save(target_path+'/wuwi_validation_neg.npy',valid_neg)\n",
    "np.save(target_path+'/wuwi_testing_pos.npy',test_pos)\n",
    "np.save(target_path+'/wuwi_testing_neg.npy',test_neg)\n",
    "\n",
    "print('Generating interaction dict.npy done.')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
